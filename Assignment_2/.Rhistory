train_dtm_freq <- train_dtm[ ,freq_words]
test_dtm_freq <- test_dtm[, freq_words]
convert_values <- function(x){
x <- ifelse(x>0, "yes", "no")
return(x)
}
train <- apply(train_dtm_freq, MARGIN = 2, convert_values)
test <- apply(test_dtm_freq, MARGIN = 2, convert_values)
NV_classifier <- naiveBayes(train, train.data$label)
NV_pred <- predict(NV_classifier, test)
confusionMatrix(data = NV_pred, reference = test.data$label, positive = "Thruthful", dnn = c("prediction", "actual"))
length(NV_pred)
dim(test_dtm)
dim(test_dtm_freq)
test_dtm_freq <- test_dtm[, freq_words]
length(freq_words)
c_data <- rbind(train.data, test,data)
c_data <- rbind(train.data, test,data)
c_corpus <- VCorpus(VectorSource(c_data))
c_data <- rbind(train.data, test,data)
dim(test.data)
dim(train.data)
c_data <- rbind(train.data, test.data)
c_corpus <- VCorpus(VectorSource(c_data))
c_dtm <- DocumentTermMatrix(c_corpus, control = list(tolower = TRUE,
removeNumbers = TRUE,
stopwords = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
fq_words <- findFrequent(c_dtm, 0.1)
findFrequent <- function(dtm, threshold){
freq_words <- findFreqTerms(x = dtm)
freq_index <- round(length(freq_words)*threshold, 0)
freq_words <- freq_words[1:freq_index]
return(freq_words)
}
fq_words <- findFrequent(c_dtm, 0.1)
str(fq_words)
train_dtm_freq <- train_dtm[ ,freq_words]
test_dtm_freq <- test_dtm[, freq_words]
print(test_dtm[, fq_words])
print(test_dtm[, 1:10])
print(test_dtm)
fq_words <- findFrequent(c_dtm, 0.1)
print(fq_words)
train_dtm_freq <- train_dtm[ ,fq_words]
train_dtm_freq <- removeSparseTerms(train_dtm, 0.9)
dim(train_dtm_freq)
train <- apply(train_dtm_freq, MARGIN = 2, convert_values)
test <- apply(test_dtm_freq, MARGIN = 2, convert_values)
train_dtm_freq <- removeSparseTerms(train_dtm, 0.9)
convert_values <- function(x){
x <- ifelse(x>0, "yes", "no")
return(x)
}
train <- apply(train_dtm_freq, MARGIN = 2, convert_values)
test <- apply(test_dtm, MARGIN = 2, convert_values)
NV_classifier <- naiveBayes(train, train.data$label)
NV_pred <- predict(NV_classifier, test)
length(NV_pred)
confusionMatrix(data = NV_pred, reference = test.data$label, positive = "Thruthful", dnn = c("prediction", "actual"))
NV_pred
table(NV_pred, test.data$label)
confusionMatrix(data = NV_pred, reference = test.data$label, positive = "Thruthful", dnn = c("prediction", "actual"))
confusionMatrix(data = NV_pred, reference = test.data$label, dnn = c("prediction", "actual"))
train_dtm_freq <- removeSparseTerms(train_dtm, 0.8)
convert_values <- function(x){
x <- ifelse(x>0, "yes", "no")
return(x)
}
train <- apply(train_dtm_freq, MARGIN = 2, convert_values)
test <- apply(test_dtm, MARGIN = 2, convert_values)
NV_classifier <- naiveBayes(train, train.data$label)
NV_pred <- predict(NV_classifier, test)
confusionMatrix(data = NV_pred, reference = test.data$label, dnn = c("prediction", "actual"))
train_dtm_freq <- removeSparseTerms(train_dtm, 0.95)
convert_values <- function(x){
x <- ifelse(x>0, "yes", "no")
return(x)
}
train <- apply(train_dtm_freq, MARGIN = 2, convert_values)
test <- apply(test_dtm, MARGIN = 2, convert_values)
NV_classifier <- naiveBayes(train, train.data$label)
NV_pred <- predict(NV_classifier, test)
confusionMatrix(data = NV_pred, reference = test.data$label, dnn = c("prediction", "actual"))
train_corpus <- tm_map(train_corpus, tokenize_ngrams, n = 2, n_min = 2)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_2/Assignment_2_code.R', echo=TRUE)
train_corpus[[1]]
train_corpus[[1]]$content
# Preprocess the train data, standard preprocessing.
train_corpus <- tm_map(train_corpus, removeWords, stopwords("english"))
train_corpus[[1]]$content
train_corpus <- tm_map(train_corpus, tolower)
train_corpus <- tm_map(train_corpus, stripWhitespace)
train_corpus <- tm_map(train_corpus, removePunctuation)
train_corpus <- tm_map(train_corpus, removeNumbers)
train_corpus[[1]]$content
train_corpus[[1]]
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
# Preprocess the train data, standard preprocessing.
train_corpus <- tm_map(train_corpus, removeWords, stopwords("english"))
train_corpus <- tm_map(train_corpus, tolower)
train_corpus <- tm_map(train_corpus, stripWhitespace)
train_corpus <- tm_map(train_corpus, removePunctuation)
train_corpus <- tm_map(train_corpus, removeNumbers)
print(train_corpus[[1]])
# Stem or lemmatize document
train_corpus <- tm_map(train_corpus, stemDocument)
print(train_corpus[[1]])
# Stem or lemmatize document
train_corpus <- tm_map(train_corpus, stemDocument)
train_corpus[[1]]
train_corpus <- tm_map(train_corpus, lemmatize_strings)
train_corpus[[1]]
train_corpus <- tm_map(train_corpus, tokenize_ngrams, n = 2, n_min = 2)
train_corpus[[1]]
class(train_corpus)
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = NLP_tokenizer))
train_dtm <- DocumentTermMatrix(train_corpus)
#Turn data into a corpus
train_corpus <- VCorpus(train.data[,1])
VectorSource(
)
train_corpus <- tm_map(train_corpus, PlainTextDocument)
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = NLP_tokenizer))
train_dtm <- DocumentTermMatrix(train_corpus)
train_dtm$dimnames$Terms
library(readtext)
library(tm)
require(SnowballC)
library(textstem)
library(e1071)
library(caret)
library(textmineR)
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
test.data <-loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 5, 5)
loadData <- function(location_t, location_d, from, to){
data <- matrix(nrow = 1, ncol= 2)
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_t <- list.files(path = paste(location_t,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_t){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 1))
}
}
data <- data[-1,]
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_d <- list.files(path = paste(location_d,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_d){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 0))
}
}
colnames(data) <- c("text", "label")
return(data)
}
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
test.data <-loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 5, 5)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
# Preprocess the train data, standard preprocessing.
train_corpus <- tm_map(train_corpus, removeWords, stopwords("english"))
train_corpus <- tm_map(train_corpus, tolower)
train_corpus <- tm_map(train_corpus, stripWhitespace)
train_corpus <- tm_map(train_corpus, removePunctuation)
train_corpus <- tm_map(train_corpus, removeNumbers)
train_corpus <- tm_map(train_corpus, )
# Stem or lemmatize document
train_corpus <- tm_map(train_corpus, stemDocument)
BiGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
train_corpus <- tm_map(train_corpus, PlainTextDocument)
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = NLP_tokenizer))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
library(textmineR)
library(readtext)
library(tm)
library(textmineR)
require(textmineR)
detach("package:NLP", unload = TRUE)
library(NLP)
detach("package:quanteda", unload = TRUE)
detach("package:SnowballC", unload = TRUE)
library(SnowballC)
detach("package:sylly", unload = TRUE)
library(sylly)
detach("package:textstem", unload = TRUE)
library(textstem)
detach("package:tokenizers", unload = TRUE)
detach("package:SnowballC", unload = TRUE)
.libPaths()
setwd("C:/Program Files/R/R-3.6.1/library"  )
install.packages("textmineR")
install.packages("textmineR")
library(textmineR)
library(textmineR)
require(textmineR)
setwd("C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_2")
library(textmineR)
loadData <- function(location_t, location_d, from, to){
data <- matrix(nrow = 1, ncol= 2)
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_t <- list.files(path = paste(location_t,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_t){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 1))
}
}
data <- data[-1,]
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_d <- list.files(path = paste(location_d,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_d){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 0))
}
}
colnames(data) <- c("text", "label")
return(data)
}
BiGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
library(readtext)
library(tm)
library(textstem)
library(e1071)
library(caret)
library(textmineR)
NGramTokenizer(lol)
install.packages("textmineR")
install.packages("textmineR")
install.packages("textmineR")
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
library(readtext)
library(tm)
library(e1071)
library(caret)
library(textmineR)
loadData <- function(location_t, location_d, from, to){
data <- matrix(nrow = 1, ncol= 2)
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_t <- list.files(path = paste(location_t,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_t){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 1))
}
}
data <- data[-1,]
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_d <- list.files(path = paste(location_d,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_d){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 0))
}
}
colnames(data) <- c("text", "label")
return(data)
}
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
test <- CreateDtm(train_corpus, ngram_window = c(2,2), stopword_vec = c(stopwords::stopwords("en"), lower= TRUE, removePunctuation = TRUE))
print(test[[1]])
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = NLP_tokenizer))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer, ngram = 2))
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = NLP_tokenizer))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(ngram = 2))
train_dtm$dimnames$Terms
test <- CreateDtm(train_corpus, ngram_window = c(2,2), stopword_vec = c(stopwords::stopwords("en"), lower= TRUE, removePunctuation = TRUE))
print(test)
print(test[1])
print(test[1,])
BiGramTokenizer <- function(x) ngrams(x, n = 2:2)
# test <- CreateDtm(train_corpus, ngram_window = c(2,2) stopword_vec = c(stopwords::stopwords("en"), lower= TRUE, removePunctuation = TRUE))
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = NLP_tokenizer))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
test.data <-loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 5, 5)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
train_corpus <- tm_map(train_corpus, tolower)
train_corpus <- tm_map(train_corpus, stripWhitespace)
BiGramTokenizer <- function(x) ngrams(words(x), n = 2:2)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
loadData <- function(location_t, location_d, from, to){
data <- matrix(nrow = 1, ncol= 2)
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_t <- list.files(path = paste(location_t,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_t){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 1))
}
}
data <- data[-1,]
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_d <- list.files(path = paste(location_d,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_d){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 0))
}
}
colnames(data) <- c("text", "label")
return(data)
}
BiGramTokenizer <- function(x) ngrams(words(x), n = 2:2)
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
test.data <-loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 5, 5)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
# Preprocess the train data, standard preprocessing.
train_corpus <- tm_map(train_corpus, removeWords, stopwordws("english"))
train_corpus <- tm_map(train_corpus, tolower)
train_corpus <- tm_map(train_corpus, stripWhitespace)
train_corpus <- tm_map(train_corpus, removePunctuation)
train_corpus <- tm_map(train_corpus, removeNumbers)
train_corpus <- tm_map(train_corpus, )
train_corpus <- tm_map(train_corpus, PlainTextDocument)
train_corpus[[1]]
# test <- CreateDtm(train_corpus, ngram_window = c(2,2) stopword_vec = c(stopwords::stopwords("en"), lower= TRUE, removePunctuation = TRUE))
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = NLP_tokenizer))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[1:10]
train_dtm$dimnames$Terms[10:100]
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
# Preprocess the train data, standard preprocessing.
train_corpus <- tm_map(train_corpus, removeWords, stopwordws("english"))
train_corpus <- tm_map(train_corpus, tolower)
train_corpus <- tm_map(train_corpus, stripWhitespace)
train_corpus <- tm_map(train_corpus, removePunctuation)
train_corpus <- tm_map(train_corpus, removeNumbers)
train_corpus <- tm_map(train_corpus, )
train_corpus <- tm_map(train_corpus, lemmatize_strings)
train_corpus <- tm_map(train_corpus, tokenize_ngrams, n = 2, n_min = 2)
train_corpus[[1]]
test <- CreateDtm(train_corpus, stopword_vec = c(stopwords::stopwords("en"), lower= TRUE, removePunctuation = TRUE, removeNumbers = TRUE,
stripWhitespace = True, lemmatize_words = TRUE, ngram_window = c(2,2)))
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
test <- CreateDtm(train_corpus, stopword_vec = c(stopwords::stopwords("en"), lower= TRUE, removePunctuation = TRUE, removeNumbers = TRUE,
stripWhitespace = True, lemmatize_words = TRUE, ngram_window = c(2,2)))
test <- CreateDtm(train_corpus, stopword_vec = c(stopwords::stopwords("en"), lower= TRUE, removePunctuation = TRUE, removeNumbers = TRUE,
stripWhitespace = TRUE, lemmatize_words = TRUE, ngram_window = c(2,2)))
train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, ngrams_window = c(2,2)))
train_dtm$dimnames$Terms[10:100]
train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = ngrams, n = 2:2))
library(readtext)
library(tm)
library(textstem)
library(e1071)
library(caret)
library(textmineR)
library(RWeka)
version
Sys.info()[["machine"]]
library(RWeka)
loadData <- function(location_t, location_d, from, to){
data <- matrix(nrow = 1, ncol= 2)
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_t <- list.files(path = paste(location_t,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_t){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 1))
}
}
data <- data[-1,]
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_d <- list.files(path = paste(location_d,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_d){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 0))
}
}
colnames(data) <- c("text", "label")
return(data)
}
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
test.data <-loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 5, 5)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = tokenize = function(x) NGramTokenizer(x,
Weka_control(min=2,
max=2))))
train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm$dimnames$Terms[10:100]
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
test.data <-loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 5, 5)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
# Preprocess the train data, standard preprocessing.
train_corpus <- tm_map(train_corpus, removeWords, stopwordws("english"))
train_corpus <- tm_map(train_corpus, tolower)
train_corpus <- tm_map(train_corpus, stripWhitespace)
train_corpus <- tm_map(train_corpus, removePunctuation)
train_corpus <- tm_map(train_corpus, removeNumbers)
# Preprocess the train data, standard preprocessing.
train_corpus <- tm_map(train_corpus, removeWords, stopwords("english"))
train_corpus <- tm_map(train_corpus, lemmatize_strings)
train_corpus[[1]]
train_corpus <- tm_map(train_corpus, PlainTextDocument)
train_corpus[[1]]
BiGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2,max=2))
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
# Preprocess the train data, standard preprocessing.
train_corpus <- tm_map(train_corpus, removeWords, stopwords("english"))
train_corpus <- tm_map(train_corpus, tolower)
train_corpus <- tm_map(train_corpus, stripWhitespace)
train_corpus <- tm_map(train_corpus, removePunctuation)
train_corpus <- tm_map(train_corpus, removeNumbers)
train_corpus <- tm_map(train_corpus, lemmatize_strings)
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# Convert to plaint text document otherwise bigram document term matrix can't be made
train_corpus <- tm_map(train_corpus, PlainTextDocument)
train_corpus[[1]]
train_corpus[[1]]$content
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_corpus <- tm_map(train_corpus, BiGramTokenizer)
# Convert to plaint text document otherwise bigram document term matrix can't be made
train_corpus <- tm_map(train_corpus, PlainTextDocument)
train_corpus[[1]]$content
train_dtm <- DocumentTermMatrix(train_corpus)
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, stopwords = TRUE,removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, stopwords = 'en',removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = 'en',removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = TRUE,removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm <- DocumentTermMatrix(train_corpus)
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = TRUE,removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stripWhitespace = TRUE, re stopwords = TRUE,removeNumbers = TRUE, tokenize = BiGramTokenizer))
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = TRUE,removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- TermDocumentMatrix(train_corpus[1], control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = TRUE,removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
dim(train_dtm)
