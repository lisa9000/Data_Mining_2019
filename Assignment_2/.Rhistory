detach("package:SnowballC", unload = TRUE)
.libPaths()
setwd("C:/Program Files/R/R-3.6.1/library"  )
install.packages("textmineR")
install.packages("textmineR")
library(textmineR)
library(textmineR)
require(textmineR)
setwd("C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_2")
library(textmineR)
loadData <- function(location_t, location_d, from, to){
data <- matrix(nrow = 1, ncol= 2)
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_t <- list.files(path = paste(location_t,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_t){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 1))
}
}
data <- data[-1,]
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_d <- list.files(path = paste(location_d,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_d){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 0))
}
}
colnames(data) <- c("text", "label")
return(data)
}
BiGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
library(readtext)
library(tm)
library(textstem)
library(e1071)
library(caret)
library(textmineR)
NGramTokenizer(lol)
install.packages("textmineR")
install.packages("textmineR")
install.packages("textmineR")
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
library(readtext)
library(tm)
library(e1071)
library(caret)
library(textmineR)
loadData <- function(location_t, location_d, from, to){
data <- matrix(nrow = 1, ncol= 2)
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_t <- list.files(path = paste(location_t,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_t){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 1))
}
}
data <- data[-1,]
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_d <- list.files(path = paste(location_d,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_d){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 0))
}
}
colnames(data) <- c("text", "label")
return(data)
}
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
test <- CreateDtm(train_corpus, ngram_window = c(2,2), stopword_vec = c(stopwords::stopwords("en"), lower= TRUE, removePunctuation = TRUE))
print(test[[1]])
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = NLP_tokenizer))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer, ngram = 2))
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = NLP_tokenizer))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(ngram = 2))
train_dtm$dimnames$Terms
test <- CreateDtm(train_corpus, ngram_window = c(2,2), stopword_vec = c(stopwords::stopwords("en"), lower= TRUE, removePunctuation = TRUE))
print(test)
print(test[1])
print(test[1,])
BiGramTokenizer <- function(x) ngrams(x, n = 2:2)
# test <- CreateDtm(train_corpus, ngram_window = c(2,2) stopword_vec = c(stopwords::stopwords("en"), lower= TRUE, removePunctuation = TRUE))
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = NLP_tokenizer))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
test.data <-loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 5, 5)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
train_corpus <- tm_map(train_corpus, tolower)
train_corpus <- tm_map(train_corpus, stripWhitespace)
BiGramTokenizer <- function(x) ngrams(words(x), n = 2:2)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
loadData <- function(location_t, location_d, from, to){
data <- matrix(nrow = 1, ncol= 2)
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_t <- list.files(path = paste(location_t,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_t){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 1))
}
}
data <- data[-1,]
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_d <- list.files(path = paste(location_d,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_d){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 0))
}
}
colnames(data) <- c("text", "label")
return(data)
}
BiGramTokenizer <- function(x) ngrams(words(x), n = 2:2)
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
test.data <-loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 5, 5)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
# Preprocess the train data, standard preprocessing.
train_corpus <- tm_map(train_corpus, removeWords, stopwordws("english"))
train_corpus <- tm_map(train_corpus, tolower)
train_corpus <- tm_map(train_corpus, stripWhitespace)
train_corpus <- tm_map(train_corpus, removePunctuation)
train_corpus <- tm_map(train_corpus, removeNumbers)
train_corpus <- tm_map(train_corpus, )
train_corpus <- tm_map(train_corpus, PlainTextDocument)
train_corpus[[1]]
# test <- CreateDtm(train_corpus, ngram_window = c(2,2) stopword_vec = c(stopwords::stopwords("en"), lower= TRUE, removePunctuation = TRUE))
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = NLP_tokenizer))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[1:10]
train_dtm$dimnames$Terms[10:100]
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
# Preprocess the train data, standard preprocessing.
train_corpus <- tm_map(train_corpus, removeWords, stopwordws("english"))
train_corpus <- tm_map(train_corpus, tolower)
train_corpus <- tm_map(train_corpus, stripWhitespace)
train_corpus <- tm_map(train_corpus, removePunctuation)
train_corpus <- tm_map(train_corpus, removeNumbers)
train_corpus <- tm_map(train_corpus, )
train_corpus <- tm_map(train_corpus, lemmatize_strings)
train_corpus <- tm_map(train_corpus, tokenize_ngrams, n = 2, n_min = 2)
train_corpus[[1]]
test <- CreateDtm(train_corpus, stopword_vec = c(stopwords::stopwords("en"), lower= TRUE, removePunctuation = TRUE, removeNumbers = TRUE,
stripWhitespace = True, lemmatize_words = TRUE, ngram_window = c(2,2)))
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
test <- CreateDtm(train_corpus, stopword_vec = c(stopwords::stopwords("en"), lower= TRUE, removePunctuation = TRUE, removeNumbers = TRUE,
stripWhitespace = True, lemmatize_words = TRUE, ngram_window = c(2,2)))
test <- CreateDtm(train_corpus, stopword_vec = c(stopwords::stopwords("en"), lower= TRUE, removePunctuation = TRUE, removeNumbers = TRUE,
stripWhitespace = TRUE, lemmatize_words = TRUE, ngram_window = c(2,2)))
train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, ngrams_window = c(2,2)))
train_dtm$dimnames$Terms[10:100]
train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = ngrams, n = 2:2))
library(readtext)
library(tm)
library(textstem)
library(e1071)
library(caret)
library(textmineR)
library(RWeka)
version
Sys.info()[["machine"]]
library(RWeka)
loadData <- function(location_t, location_d, from, to){
data <- matrix(nrow = 1, ncol= 2)
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_t <- list.files(path = paste(location_t,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_t){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 1))
}
}
data <- data[-1,]
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_d <- list.files(path = paste(location_d,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_d){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 0))
}
}
colnames(data) <- c("text", "label")
return(data)
}
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
test.data <-loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 5, 5)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = tokenize = function(x) NGramTokenizer(x,
Weka_control(min=2,
max=2))))
train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm$dimnames$Terms[10:100]
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
test.data <-loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 5, 5)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
# Preprocess the train data, standard preprocessing.
train_corpus <- tm_map(train_corpus, removeWords, stopwordws("english"))
train_corpus <- tm_map(train_corpus, tolower)
train_corpus <- tm_map(train_corpus, stripWhitespace)
train_corpus <- tm_map(train_corpus, removePunctuation)
train_corpus <- tm_map(train_corpus, removeNumbers)
# Preprocess the train data, standard preprocessing.
train_corpus <- tm_map(train_corpus, removeWords, stopwords("english"))
train_corpus <- tm_map(train_corpus, lemmatize_strings)
train_corpus[[1]]
train_corpus <- tm_map(train_corpus, PlainTextDocument)
train_corpus[[1]]
BiGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2,max=2))
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
# Preprocess the train data, standard preprocessing.
train_corpus <- tm_map(train_corpus, removeWords, stopwords("english"))
train_corpus <- tm_map(train_corpus, tolower)
train_corpus <- tm_map(train_corpus, stripWhitespace)
train_corpus <- tm_map(train_corpus, removePunctuation)
train_corpus <- tm_map(train_corpus, removeNumbers)
train_corpus <- tm_map(train_corpus, lemmatize_strings)
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# Convert to plaint text document otherwise bigram document term matrix can't be made
train_corpus <- tm_map(train_corpus, PlainTextDocument)
train_corpus[[1]]
train_corpus[[1]]$content
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_corpus <- tm_map(train_corpus, BiGramTokenizer)
# Convert to plaint text document otherwise bigram document term matrix can't be made
train_corpus <- tm_map(train_corpus, PlainTextDocument)
train_corpus[[1]]$content
train_dtm <- DocumentTermMatrix(train_corpus)
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, stopwords = TRUE,removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, stopwords = 'en',removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = 'en',removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = TRUE,removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm <- DocumentTermMatrix(train_corpus)
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = TRUE,removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stripWhitespace = TRUE, re stopwords = TRUE,removeNumbers = TRUE, tokenize = BiGramTokenizer))
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = TRUE,removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- TermDocumentMatrix(train_corpus[1], control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = TRUE,removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
dim(train_dtm)
library(readtext)
library(tm)
library(textstem)
library(e1071)
library(caret)
library(textmineR)
library(RWeka)
loadData <- function(location_t, location_d, from, to){
data <- matrix(nrow = 1, ncol= 2)
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_t <- list.files(path = paste(location_t,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_t){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 1))
}
}
data <- data[-1,]
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_d <- list.files(path = paste(location_d,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_d){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 0))
}
}
colnames(data) <- c("text", "label")
return(data)
}
BiGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2,max=2))
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
test.data <-loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 5, 5)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
# Preprocess the train data, standard preprocessing.
train_corpus <- tm_map(train_corpus, removeWords, stopwords("english"))
train_corpus <- tm_map(train_corpus, tolower)
train_corpus <- tm_map(train_corpus, stripWhitespace)
train_corpus <- tm_map(train_corpus, removePunctuation)
train_corpus <- tm_map(train_corpus, removeNumbers)
train_corpus <- tm_map(train_corpus, lemmatize_strings)
train_corpus <- tm_map(train_corpus, BiGramTokenizer)
train_corpus[[1]]$content
# Convert to plaint text document otherwise bigram document term matrix can't be made
train_corpus <- tm_map(train_corpus, PlainTextDocument)
train_corpus[[1]]$content
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- TermDocumentMatrix(train_corpus[1], control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = TRUE,removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize = function(x) NGramTokenizer(x, Weka_control(min=2,max=2))))
train_dtm <- TermDocumentMatrix(train_corpus[1], control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = TRUE, removeNumbers = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# train_dtm <- DocumentTermMatrix(train_corpus[1], control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
# removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE)))
train_dtm <- TermDocumentMatrix(train_corpus[1], control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = TRUE, removeNumbers = TRUE, lemmatize_words = TRUE, tokenize = BiGramTokenizer))
train_dtm$dimnames$Terms[10:100]
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE))
# For bigram
train_dtm_bi <- TermDocumentMatrix(train_corpus[1], control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = TRUE, removeNumbers = TRUE, lemmatize_words = TRUE, tokenize = BiGramTokenizer))
train_dtm_bi$dimnames$Terms[10:100]
# For bigram
train_dtm_bi <- TermDocumentMatrix(train_corpus, control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = TRUE, removeNumbers = TRUE, lemmatize_words = TRUE, tokenize = BiGramTokenizer))
train_dtm_bi$dimnames$Terms[10:100]
dim(train_dtm_bi)
train_dtm_bi$dimnames$Terms[1:10]
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, weighting = weightTfIdf))
test_corpus <- VCorpus(VectorSource(test.data$text))
test_dtm <- DocumentTermMatrix(test_corpus,control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, weighting = weightTfIdf))
train_dtm_freq_uni <- removeSparseTerms(train_dtm_uni, 0.95)
convert_values <- function(x){
x <- ifelse(x>0, "yes", "no")
return(x)
}
dim(train_dtm_freq_uni)
train <- apply(train_dtm_freq_uni, MARGIN = 2, convert_values)
test <- apply(test_dtm, MARGIN = 2, convert_values)
NV_classifier <- naiveBayes(train, train.data$label)
NV_classifier <- naiveBayes(train, train.data[,2])
NV_pred <- predict(NV_classifier, test)
confusionMatrix(data = NV_pred, reference = test.data$label, positive = "1",dnn = c("prediction", "actual"))
confusionMatrix(data = NV_pred, reference = test.data[,2], positive = "1",dnn = c("prediction", "actual"))
NV_pred
train
test.data[,2]
class(test.data[,2])
NV_classifier <- naiveBayes(train, as.numeric(train.data[,2]))
NV_pred <- predict(NV_classifier, test)
confusionMatrix(data = NV_pred, reference = test.data[,2], positive = "1",dnn = c("prediction", "actual"))
NV_classifier <- naiveBayes(train, as.factor(train.data[,2]))
NV_pred <- predict(NV_classifier, test)
NV_pred
confusionMatrix(data = NV_pred, reference = test.data[,2], positive = "1",dnn = c("prediction", "actual"))
confusionMatrix(data = NV_pred, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE))
test_dtm <- DocumentTermMatrix(test_corpus,control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE))
train_dtm_freq_uni <- removeSparseTerms(train_dtm_uni, 0.95)
train <- apply(train_dtm_freq_uni, MARGIN = 2, convert_values)
test <- apply(test_dtm, MARGIN = 2, convert_values)
NV_classifier <- naiveBayes(train, as.factor(train.data[,2]))
NV_pred <- predict(NV_classifier, test)
confusionMatrix(data = NV_pred, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, weighting = weightTf))
test_dtm <- DocumentTermMatrix(test_corpus,control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE))
train_dtm_freq_uni <- removeSparseTerms(train_dtm_uni, 0.95)
dim(train_dtm_freq_uni)
dim(train_dtm_freq_uni$dimnames$Terms)
train_dtm_uni$dimnames$Terms[1:10]
train_dtm_freq_uni <- removeSparseTerms(train_dtm_uni, 0.95)
dim(train_dtm_freq_uni$dimnames$Terms)
dim(train_dtm_uni)
train_dtm_freq_uni <- removeSparseTerms(train_dtm_uni, 0.95)
dim(train_dtm_freq_uni$dimnames$Terms)
train_dtm_freq_uni$dimnames$Terms
train <- apply(train_dtm_freq_uni, MARGIN = 2, convert_values)
test <- apply(test_dtm, MARGIN = 2, convert_values)
NV_classifier <- naiveBayes(train, as.factor(train.data[,2]))
NV_pred <- predict(NV_classifier, test)
confusionMatrix(data = NV_pred, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
train_dtm_freq_uni <- removeSparseTerms(train_dtm_uni, 0.98)
train <- apply(train_dtm_freq_uni, MARGIN = 2, convert_values)
test <- apply(test_dtm, MARGIN = 2, convert_values)
NV_classifier <- naiveBayes(train, as.factor(train.data[,2]))
NV_pred <- predict(NV_classifier, test)
confusionMatrix(data = NV_pred, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
dim(train_dtm_uni)
train_dtm_freq_uni <- removeSparseTerms(train_dtm_uni, 0.99)
train <- apply(train_dtm_freq_uni, MARGIN = 2, convert_values)
test <- apply(test_dtm, MARGIN = 2, convert_values)
NV_classifier <- naiveBayes(train, as.factor(train.data[,2]))
NV_pred <- predict(NV_classifier, test)
confusionMatrix(data = NV_pred, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
train_dtm_freq_uni <- removeSparseTerms(train_dtm_uni, 0.975)
train <- apply(train_dtm_freq_uni, MARGIN = 2, convert_values)
test <- apply(test_dtm, MARGIN = 2, convert_values)
NV_classifier <- naiveBayes(train, as.factor(train.data[,2]))
NV_pred <- predict(NV_classifier, test)
confusionMatrix(data = NV_pred, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
test_dtm_bi <- DocumentTermMatrix(test_corpus,control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize= BiGramTokenizer))
test_dtm_bi <- DocumentTermMatrix(test_corpus,control = list(tolower = TRUE, stopwords = TRUE, removePunctuation = TRUE,
removeNumbers = TRUE, stripWhitespace = TRUE, lemmatize_words = TRUE, tokenize= BiGramTokenizer))
train_dtm_freq_bi <- removeSparseTerms(train_dtm_bi, 0.95)
train_bi <- apply(train_dtm_freq_bi, MARGIN = 2, convert_values)
test_bi <- apply(test_dtm_bi, MARGIN = 2, convert_values)
# Naive bayes classifier Bigram
NV_classifier <- naiveBayes(train_bi, as.factor(train.data[,2]))
# Naive bayes classifier Bigram
NV_classifier <- naiveBayes(train_bi, as.factor(train.data[,2]))
NV_pred <- predict(NV_classifier, test_bi)
train_bi
dim(train_dtm_freq_bi)
# For bigram
train_dtm_bi <- DocumentTermMatrix(train_corpus, control = list(tolower = TRUE, stripWhitespace = TRUE, stopwords = TRUE, removeNumbers = TRUE, lemmatize_words = TRUE, tokenize = BiGramTokenizer))
train_dtm_freq_bi <- removeSparseTerms(train_dtm_bi, 0.95)
train_bi <- apply(train_dtm_freq_bi, MARGIN = 2, convert_values)
test_bi <- apply(test_dtm_bi, MARGIN = 2, convert_values)
# Naive bayes classifier Bigram
NV_classifier_bi <- naiveBayes(train_bi, as.factor(train.data[,2]))
NV_pred_bi <- predict(NV_classifier, test_bi)
confusionMatrix(data = NV_pred_bi, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
test_bi <- apply(test_dtm, MARGIN = 2, convert_values)
# Naive bayes classifier Bigram
NV_classifier_bi <- naiveBayes(train_bi, as.factor(train.data[,2]))
NV_pred_bi <- predict(NV_classifier, test_bi)
# ConfusionMatrixes
confusionMatrix(data = NV_pred, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
confusionMatrix(data = NV_pred_bi, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
# Naive bayes classifier Bigram
NV_classifier_bi <- naiveBayes(train_bi, as.factor(train.data[,2]))
test_bi <- apply(test_dtm_bi, MARGIN = 2, convert_values)
# Naive bayes classifier Bigram
NV_classifier_bi <- naiveBayes(train_bi, as.factor(train.data[,2]))
NV_pred_bi <- predict(NV_classifier_bi, test_bi)
confusionMatrix(data = NV_pred_bi, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
train_dtm_freq_bi <- removeSparseTerms(train_dtm_bi, 0.99)
# Create train and test categorical Biram
train_bi <- apply(train_dtm_freq_bi, MARGIN = 2, convert_values)
test_bi <- apply(test_dtm_bi, MARGIN = 2, convert_values)
# Naive bayes classifier Bigram
NV_classifier_bi <- naiveBayes(train_bi, as.factor(train.data[,2]))
NV_pred_bi <- predict(NV_classifier_bi, test_bi)
confusionMatrix(data = NV_pred_bi, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
