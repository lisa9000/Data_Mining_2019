test.data[,2]
as.numeric(test.data[,2])
y_true <- as.numeric(test.data[,2])
F1_Score(y_true, as.numeric(data.frame(NV_pred)), positive = "Truthfull")
F1_Score(y_true, as.numeric(NV_pred), positive = "Truthfull")
NV_pred
as.numeric(NV_pred)
y_data_uni <- as.numeric(NV_pred)
F1_Score(y_true, y_data_uni, positive = "Truthfull")
# ConfusionMatrices
cm_uni <- confusionMatrix(data = NV_pred, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
cm_uni$dots
cm_uni
# ConfusionMatrices
result <- confusionMatrix(data = NV_pred, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"), mode = 'prec_recall')
result$byClass["Precision"]
result$byClass[c("Precision", "recall", "accuracy", "F1")]
result$byClass[c("Precision", "Recall", "Accuracy", "F1")]
result$byClass[c("Precision", "Recall", "F1")]
# ConfusionMatrices
result_uni <- confusionMatrix(data = NV_pred, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"), mode = 'prec_recall')
result_uni$byClass[c("Precision", "Recall", "F1")]
restul_uni
result_uni
result_uni$byClass
result_uni$positive
result_uni$overall
result_uni$overall["Accuracy"]
result_bi <- confusionMatrix(data = NV_pred_bi, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
result_uni$overall["Accuracy"]
result_uni$byClass[c("Precision", "Recall", "F1")]
result_bi$overall["Accuracy"]
result_bi$byClass[c("Precision", "Recall", "F1")]
# Feature selection with mutual information
# compute mutual information of each term with class label
train_dtm_uni <- apply(as.matrix(train_dtm_uni),2, function(x,y){mi.plugin(table(x,y)/length(y))},train.data[,2])
train_dtm_uni_order <- order(train_dtm_uni, decreasing = T)
train_dtm_uni[train_dtm_uni_order[1:10]]
train_dtm_uni[train_dtm_uni_order[1:5]]
train_dtm_bi <- apply(as.matrix(train_dtm_bi),2, function(x,y){mi.plugin(table(x,y)/length(y))},train.data[,2])
train_dtm_bi_order <- order(train_dtm_bi, decreasing = T)
train_dtm_bi[train_dtm_bi_order[1:10]]
test_dtm_uni <- DocumentTermMatrix(test_corpus, control = list(stopwords = stopwords('en'),removePunctuation = T, tolower = tolower, stripWhitespace = TRUE,removeNumbers = T,
)
]
test_dtm_uni <- DocumentTermMatrix(test_corpus, control = list(stopwords = stopwords('en'),removePunctuation = T, tolower = tolower, stripWhitespace = TRUE,removeNumbers = T,
stemming = lemmatize_words, dictionary=dimnames(train_dtm_uni[train_dtm_uni_order[1:50]])[[2]]))
test_dtm_bi <- DocumentTermMatrix(test_corpus,control = list(stopwords = stopwords('en'), removePunctuation = TRUE, tolower = TRUE, stripWhitespace = T ,removeNumbers = TRUE ,
stemming = lemmatize_words(wo), tokenize = BiGramTokenizer, dictionary=dimnames(train_dtm_bi[train_dtm_bi_order[1:50]])[[2]]))
test_dtm_bi <- DocumentTermMatrix(test_corpus,control = list(stopwords = stopwords('en'), removePunctuation = TRUE, tolower = TRUE, stripWhitespace = T ,removeNumbers = TRUE ,
stemming = lemmatize_words, tokenize = BiGramTokenizer, dictionary=dimnames(train_dtm_bi[train_dtm_bi_order[1:50]])[[2]]))
# Naive Bayes classiffier unigram
NV_classifier <- multinomial_naive_bayes(as.matrix(train_dtm_uni), as.factor(train.data[,2]))
NV_pred <- predict(NV_classifier, as.matrix(test_dtm_uni))
# Naive Bayes classiffier unigram
NV_classifier <- multinomial_naive_bayes(train_dtm_uni, as.factor(train.data[,2]))
train_dtm_bi
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus, control = list(stopwords = stopwords('en'),removePunctuation = T, tolower = tolower, stripWhitespace = TRUE,removeNumbers = T,
stemming = lemmatize_words))
# For bigram
train_dtm_bi <- DocumentTermMatrix(train_corpus, control = list( stopwords = stopwords('en'), removePunctuation = TRUE, tolower = TRUE, stripWhitespace = T ,removeNumbers = TRUE ,
stemming = lemmatize_words, tokenize = BiGramTokenizer))
# # Remove sparse terms
train_dtm_uni <- removeSparseTerms(train_dtm_uni, 0.99)
train_dtm_bi <- removeSparseTerms(train_dtm_bi, 0.99)
# Feature selection with mutual information
# compute mutual information of each term with class label
train_mi_uni <- apply(as.matrix(train_mi_uni),2, function(x,y){mi.plugin(table(x,y)/length(y))},train.data[,2])
train_mi_uni_order <- order(train_mi_uni, decreasing = T)
train_mi_uni[train_mi_uni_order[1:10]]
# Feature selection with mutual information
# compute mutual information of each term with class label
train_mi_uni <- apply(as.matrix(train_dtm_uni),2, function(x,y){mi.plugin(table(x,y)/length(y))},train.data[,2])
train_mi_uni_order <- order(train_mi_uni, decreasing = T)
train_mi_uni[train_mi_uni_order[1:10]]
train_mi_bi <- apply(as.matrix(train_dtm_bi),2, function(x,y){mi.plugin(table(x,y)/length(y))},train.data[,2])
train_mi_bi_order <- order(train_mi_bi, decreasing = T)
train_mi_bi[train_mi_bi_order[1:10]]
train_dtm_bi[train_mi_bi_order[1:10]]
# Create a test set for the unigram and bigram
test_corpus <- VCorpus(VectorSource(test.data[,1]))
train_dtm_bi
test_dtm_uni <- DocumentTermMatrix(test_corpus, control = list(stopwords = stopwords('en'),removePunctuation = T, tolower = tolower, stripWhitespace = TRUE,removeNumbers = T,
stemming = lemmatize_words, dictionary=dimnames(train_dtm_uni[train_mi_uni_order[1:50]])[[2]]))
test_dtm_uni <- DocumentTermMatrix(test_corpus, control = list(stopwords = stopwords('en'),removePunctuation = T, tolower = tolower, stripWhitespace = TRUE,removeNumbers = T,
stemming = lemmatize_words, dictionary=dimnames(train_dtm_uni[,train_mi_uni_order[1:50]])[[2]]))
test_dtm_bi
test_dtm_bi <- DocumentTermMatrix(test_corpus,control = list(stopwords = stopwords('en'), removePunctuation = TRUE, tolower = TRUE, stripWhitespace = T ,removeNumbers = TRUE ,
stemming = lemmatize_words, tokenize = BiGramTokenizer, dictionary=dimnames(train_dtm_bi[,train_mi_bi_order[1:50]])[[2]]))
test_dtm_bi
# Naive Bayes classiffier unigram
NV_classifier <- multinomial_naive_bayes(train_dtm_uni, as.factor(train.data[,2]))
NV_pred <- predict(NV_classifier, as.matrix(test_dtm_uni))
# Naive Bayes classiffier unigram
NV_classifier <- multinomial_naive_bayes(as.matrix(train_dtm_uni), as.factor(train.data[,2]))
NV_pred <- predict(NV_classifier, as.matrix(test_dtm_uni))
test_dtm_uni
# Naive Bayes classiffier unigram
NV_classifier <- multinomial_naive_bayes(as.matrix(train_dtm_uni[, train_mi_uni_order[1:50]]), as.factor(train.data[,2]))
NV_pred <- predict(NV_classifier, as.matrix(test_dtm_uni))
# ConfusionMatrices
result_uni <- confusionMatrix(data = NV_pred, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"), mode = 'prec_recall')
result_uni$overall["Accuracy"]
result_uni$byClass[c("Precision", "Recall", "F1")]
# Naive bayes classifier Bigram
NV_classifier_bi <- multinomial_naive_bayes(as.matrix(train_dtm_bi[, train_mi_bi_order[1:50]]), as.factor(train.data[,2]))
NV_pred_bi <- predict(NV_classifier_bi, as.matrix(test_dtm_bi))
result_bi <- confusionMatrix(data = NV_pred_bi, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
result_bi$overall["Accuracy"]
result_bi$byClass[c("Precision", "Recall", "F1")]
test_dtm_uni <- DocumentTermMatrix(test_corpus, control = list(stopwords = stopwords('en'),removePunctuation = T, tolower = tolower, stripWhitespace = TRUE,removeNumbers = T,
stemming = lemmatize_words, dictionary=dimnames(train_dtm_uni)[[2]]))
test_dtm_bi <- DocumentTermMatrix(test_corpus,control = list(stopwords = stopwords('en'), removePunctuation = TRUE, tolower = TRUE, stripWhitespace = T ,removeNumbers = TRUE ,
stemming = lemmatize_words, tokenize = BiGramTokenizer, dictionary=dimnames(train_dtm_bi)[[2]]))
test_dtm_uni
# Naive Bayes classiffier unigram
NV_classifier <- multinomial_naive_bayes(as.matrix(train_dtm_uni), as.factor(train.data[,2]))
NV_pred <- predict(NV_classifier, as.matrix(test_dtm_uni))
# Naive bayes classifier Bigram
NV_classifier_bi <- multinomial_naive_bayes(as.matrix(train_dtm_bi), as.factor(train.data[,2]))
NV_pred_bi <- predict(NV_classifier_bi, as.matrix(test_dtm_bi))
# ConfusionMatrices
result_uni <- confusionMatrix(data = NV_pred, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"), mode = 'prec_recall')
result_bi <- confusionMatrix(data = NV_pred_bi, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
result_uni$overall["Accuracy"]
result_uni$byClass[c("Precision", "Recall", "F1")]
result_bi$overall["Accuracy"]
result_bi$byClass[c("Precision", "Recall", "F1")]
install.packages("glmnet")
library(glmnet)
reviews.glmnet <- cv.glmnet(as.matrix(train_dtm_uni), as.numeric(train.data[,2]), family = "binomial", type.measure = "class")
plot(reviews.glmnet)
coef(reviews.glmnet, s="lambda.1se")
plot(reviews.glmnet)
# # Remove sparse terms
train_dtm_uni <- removeSparseTerms(train_dtm_uni, 0.95)
reviews.glmnet <- cv.glmnet(as.matrix(train_dtm_uni), as.numeric(train.data[,2]), family = "binomial", type.measure = "class")
plot(reviews.glmnet)
# # Remove sparse terms
train_dtm_uni <- removeSparseTerms(train_dtm_uni, 0.999)
reviews.glmnet <- cv.glmnet(as.matrix(train_dtm_uni), as.numeric(train.data[,2]), family = "binomial", type.measure = "class")
plot(reviews.glmnet)
# # Remove sparse terms
train_dtm_uni <- removeSparseTerms(train_dtm_uni, 0.99)
reviews.glmnet <- cv.glmnet(as.matrix(train_dtm_uni), as.numeric(train.data[,2]), family = "binomial", type.measure = "class")
plot(reviews.glmnet)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_2/Assignment_2_code_LR.R', echo=TRUE)
dim(train_dtm_uni)
coef.cv.glmnet(reviews.glmnet, s="lambda.1se")
coef.cv.glmnet(reviews.glmnet, s="lambda.1se")[1:10]
coef.cv.glmnet(reviews.glmnet, s="lambda.1se")[1:10,1:10]
coef.cv.glmnet(reviews.glmnet, s="lambda.1se")[1]
coef.cv.glmnet(reviews.glmnet, s="lambda.1se")[1,]
coef.cv.glmnet(reviews.glmnet, s="lambda.1se")[,1]
reviews.logreg.pred <- predict(reviews.glmnet, newx = as.matrix(test_dtm_uni), s="lambda.1se", type="Class")
reviews.logreg.pred <- predict(reviews.glmnet, newx = as.matrix(test_dtm_uni), s="lambda.1se", type="class")
confusionMatrix(data = reviews.logreg.pred, refrence = as.factor(test.data[,2]))
confusionMatrix(data = reviews.logreg.pred, reference = as.factor(test.data[,2]))
confusionMatrix(data = as.factor(reviews.logreg.pred), reference = as.factor(test.data[,2]))
coef(reviews.glmnet, s="lambda.1se")
coef(reviews.glmnet, s="lambda.1se")[1]
coef(reviews.glmnet, s="lambda.1se")[1,1]
max(coef(reviews.glmnet, s="lambda.1se"))
coef(reviews.glmnet, s="lambda.1se")
# # Remove sparse terms
train_dtm_uni <- removeSparseTerms(train_dtm_uni, 0.95)
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus, control = list(stopwords = stopwords('en'),removePunctuation = T, tolower = tolower, stripWhitespace = TRUE,removeNumbers = T,
stemming = lemmatize_words))
# # Remove sparse terms
train_dtm_uni <- removeSparseTerms(train_dtm_uni, 0.95)
reviews.glmnet.uni <- cv.glmnet(as.matrix(train_dtm_uni), as.numeric(train.data[,2]), family = "binomial", type.measure = "class")
plot(reviews.glmnet)
coef(reviews.glmnet, s="lambda.1se")
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus, control = list(stopwords = stopwords('en'),removePunctuation = T, tolower = tolower, stripWhitespace = TRUE,removeNumbers = T,
stemming = lemmatize_words))
# # Remove sparse terms
train_dtm_uni <- removeSparseTerms(train_dtm_uni, 0.95)
test_dtm_uni <- DocumentTermMatrix(test_corpus, control = list(stopwords = stopwords('en'),removePunctuation = T, tolower = tolower, stripWhitespace = TRUE,removeNumbers = T,
stemming = lemmatize_words, dictionary=dimnames(train_dtm_uni)[[2]]))
dim(train_dtm_uni)
reviews.glmnet.uni <- cv.glmnet(as.matrix(train_dtm_uni), as.numeric(train.data[,2]), family = "binomial", type.measure = "class")
plot(reviews.glmnet)
plot(reviews.glmnet.uni)
coef(reviews.glmnet, s="lambda.1se")
coef(reviews.glmnet.uni, s="lambda.1se")
reviews.logreg.pred <- predict(reviews.glmnet, newx = as.matrix(test_dtm_uni), s="lambda.1se", type="class")
reviews.logreg.pred <- predict(reviews.glmnet.uni, newx = as.matrix(test_dtm_uni), s="lambda.1se", type="class")
result_uni <- confusionMatrix(data = as.factor(reviews.logreg.pred), reference = as.factor(test.data[,2]))
result_uni
reviews.glmnet.bi <- cv.glmnet(as.matrix(train_dtm_bi), as.numeric(train.data[,2]), family = "binomial", type.measure = "class")
plot(reviews.glmnet.bi)
coef(reviews.glmnet.bi, s="lambda.1se")
reviews.logreg.pred <- predict(reviews.glmnet.bi, newx = as.matrix(test_dtm_bi), s="lambda.1se", type="class")
result_bi <- confusionMatrix(data = as.factor(reviews.logreg.pred), reference = as.factor(test.data[,2]))
result_uni
result_uni
result_uni
result_bi
result_uni$byClass
result_bi$byClass
plot(reviews.glmnet.uni$glmnet.fit)
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
test.data <-loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 5, 5)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus, control = list(stopwords = stopwords('en'),removePunctuation = T, tolower = tolower, stripWhitespace = TRUE,removeNumbers = T,
stemming = lemmatize_words))
dim(train_dtm_uni)
findFreqTerms(train_dtm_uni, 5)
fivefreq<- findFreqTerms(train_dtm_uni, 5)
fivefreq
findFreqTerms(train_dtm_uni, 10)
findFreqTerms(train_dtm_uni, 15)
findFreqTerms(train_dtm_uni, 50)
findFreqTerms(train_dtm_uni, 15)
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower))
library(dplyr)
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower))
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus, control = list(stopwords = stopwords('en'),removePunctuation = T, stripWhitespace = TRUE,removeNumbers = T,
stemming = lemmatize_words))
train_dtm_uni$nrow[1]
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus[1], control = list(stopwords = stopwords('en'),removePunctuation = T, stripWhitespace = TRUE,removeNumbers = T,
stemming = lemmatize_words))
train_dtm_uni$dimnames$Terms
train.data[1,1]
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, c(stopwords('en'), 'gucci'))
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus[1], control = list( stripWhitespace = TRUE,removeNumbers = T,
stemming = lemmatize_words))
train_dtm_uni$dimnames$Terms
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, c(stopwords("en"), 'aaa', 'aa')) %>% tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>% tm_map(stripWhitespace)
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus[1], control = list(stemming = lemmatize_words))
train_dtm_uni$dimnames$Terms
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(lemmatize_words)%>% tm_map(removeWords, c(stopwords("en"), 'aaa', 'aa')) %>% tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>% tm_map(stripWhitespace)
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus[1])
train_dtm_uni$dimnames$Terms
# For bigram
train_dtm_bi <- DocumentTermMatrix(train_corpus[1], control = list(tokenize = BiGramTokenizer))
train_dtm_bi$dimnames$Terms
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(lemmatize_words)%>% tm_map(removeWords, c(stopwords("en"), 'aaa', 'aa')) %>% tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>% tm_map(stripWhitespace)
train_corpus[[1]]$content
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(lemmatize_words)%>% tm_map(removeWords, c(stopwords("en"), 'aaa', 'aa')) %>% tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>% tm_map(stripWhitespace)
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus[1])
train_dtm_uni$dimnames$Terms
train_corpus[[1]]$content
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(lemmatize_words)%>% tm_map(removeWords, c(stopwords("en"), 'aaa', 'aa')) %>% tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>% tm_map(stripWhitespace)
train_corpus[[1]]$content
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus[1])
train_dtm_uni$dimnames$Terms
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
library(readtext)
library(tm)
library(textstem)
library(e1071)
library(caret)
library(textmineR)
library(RWeka)
library(entropy)
library(dplyr)
loadData <- function(location_t, location_d, from, to){
data <- matrix(nrow = 1, ncol= 2)
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_t <- list.files(path = paste(location_t,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_t){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 1))
}
}
data <- data[-1,]
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_d <- list.files(path = paste(location_d,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_d){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 0))
}
}
colnames(data) <- c("text", "label")
return(data)
}
BiGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2,max=2))
convert_values <- function(x){
x <- ifelse(x>0, "yes", "no")
return(x)
}
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
test.data <-loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 5, 5)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(lemmatize_words)%>% tm_map(removeWords, c(stopwords("en"), 'aaa', 'aa')) %>% tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)
train_corpus[[1]]$content
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus[1])
train_dtm_uni$dimnames$Terms
# For bigram
train_dtm_bi <- DocumentTermMatrix(train_corpus[1], control = list(tokenize = BiGramTokenizer))
train_dtm_bi$dimnames$Terms
# For bigram
train_dtm_bi <- DocumentTermMatrix(train_corpus[1], control = list(removeWords = stopwords('en'),tokenize = BiGramTokenizer))
train_dtm_bi$dimnames$Terms
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(lemmatize_words)%>% tm_map(removeWords, stopwords("en")) %>% tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)
train_corpus[[1]]$content
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, stopwords("en")) %>%tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)
train_corpus[[1]]$content
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, stopwords("en")) %>%tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)
train_corpus[[1]]$content
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, stopwords("english")) %>%tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)
train_corpus[[1]]$content
train_corpus <- train_corpus[1] %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, stopwords("english")) %>%tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)
train_corpus[[1]]$content
stopwords("en")
train_corpus <- train_corpus[1] %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, c(stopwords("english"), "th")) %>%tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)
train_corpus[[1]]$content
train_corpus <- train_corpus[1] %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, c(stopwords("english"), "th", "us")) %>%tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)
train_corpus[[1]]$content
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus[1])
# For bigram
train_dtm_bi <- DocumentTermMatrix(train_corpus[1], control = list(removeWords = stopwords('en'),tokenize = BiGramTokenizer))
train_dtm_uni$dimnames$Terms
train_dtm_bi$dimnames$Terms
# For bigram
train_dtm_bi <- DocumentTermMatrix(train_corpus[1], control = list(tokenize = BiGramTokenizer))
train_dtm_bi$dimnames$Terms
train_corpus <- train_corpus[1:3] %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, c(stopwords("english"), "th", "us")) %>%tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)
# For bigram
train_dtm_bi <- DocumentTermMatrix(train_corpus[1:3], control = list(tokenize = BiGramTokenizer))
train_corpus <- train_corpus[1:3,] %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, c(stopwords("english"), "th", "us")) %>%tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)
train_corpus <- train_corpus[1:3] %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, c(stopwords("english"), "th", "us")) %>%tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
train_corpus <- train_corpus[1:3] %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, c(stopwords("english"), "th", "us")) %>%tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)
train_corpus[[1:3]]$content
train_corpus$content
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus[1:3])
# For bigram
train_dtm_bi <- DocumentTermMatrix(train_corpus[1:3], control = list(tokenize = BiGramTokenizer))
train_dtm_uni$dimnames$Terms
train_dtm_bi$dimnames$Terms
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus)
# For bigram
train_dtm_bi <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
train_dtm_uni$dimnames$Terms
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, c(stopwords("english"), "th", "us", "aaa", "m")) %>%tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)
train_corpus$content
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus)
train_dtm_uni$dimnames$Terms
# For bigram
train_dtm_bi <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
train_dtm_bi$dimnames$Terms
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, c(stopwords("english"), "th", "us")) %>%tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)%>% removeWords(stopwords("en"))
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, c(stopwords("english"), "th", "us")) %>%tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)%>% removeWords(stopwords("en"))
train_corpus$content
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, c(stopwords("english"), "th", "us")) %>%tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)%>% removeWords(stopwords("en"))
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, c(stopwords("english"), "th", "us")) %>%tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>% tm_map(stripWhitespace)%>% tm_map(removeWords(stopwords("en")))
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, stopwords("english")) %>%
tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>%
tm_map(stripWhitespace)%>% tm_map(removeWords, c(stopwords("en"), "th", "us"))
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus)
# For bigram
train_dtm_bi <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
train_dtm_bi$dimnames$Terms
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
test.data <-loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 5, 5)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, stopwords("english")) %>%
tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>%
tm_map(stripWhitespace)%>% tm_map(removeWords, c(stopwords("en"), "th", "us", "also"))
# For unigram
train_dtm_uni <- DocumentTermMatrix(train_corpus)
# For bigram
train_dtm_bi <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
train_dtm_uni$dimnames$Terms
# # Remove sparse terms
train_dtm_uni <- removeSparseTerms(train_dtm_uni, 0.99)
train_dtm_uni$dimnames$Terms
train_dtm_bi <- removeSparseTerms(train_dtm_bi, 0.99)
# Create a test set for the unigram and bigram
test_corpus <- VCorpus(VectorSource(test.data[,1]))
test_corpus <- test_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, stopwords("english")) %>%
tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>%
tm_map(stripWhitespace)%>% tm_map(removeWords, c(stopwords("en"), "th", "us", "also"))
test_dtm_uni <- DocumentTermMatrix(test_corpus, list(dictionary=dimnames(train_dtm_uni)[[2]]))
test_dtm_bi <- DocumentTermMatrix(test_corpus,control = list(tokenize = BiGramTokenizer, dictionary=dimnames(train_dtm_bi)[[2]]))
test_dtm_bi$dimnames$Terms
# Naive Bayes classiffier unigram
NV_classifier <- multinomial_naive_bayes(as.matrix(train_dtm_uni), as.factor(train.data[,2]))
NV_pred <- predict(NV_classifier, as.matrix(test_dtm_uni))
# Naive bayes classifier Bigram
NV_classifier_bi <- multinomial_naive_bayes(as.matrix(train_dtm_bi), as.factor(train.data[,2]))
NV_pred_bi <- predict(NV_classifier_bi, as.matrix(test_dtm_bi))
# ConfusionMatrices
result_uni <- confusionMatrix(data = NV_pred, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"), mode = 'prec_recall')
result_bi <- confusionMatrix(data = NV_pred_bi, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
result_uni$overall["Accuracy"]
result_uni$byClass[c("Precision", "Recall", "F1")]
result_bi$overall["Accuracy"]
result_bi$byClass[c("Precision", "Recall", "F1")]
loadData <- function(location_t, location_d, from, to){
data <- matrix(nrow = 1, ncol= 2)
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_t <- list.files(path = paste(location_t,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_t){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 1))
}
}
data <- data[-1,]
for(i in from:to){
fold <- paste("fold", i, sep = "")
list_of_files_d <- list.files(path = paste(location_d,"/", fold,"/", sep=""), pattern = ".*.txt", full.names = TRUE, all.files = FALSE)
for(fileName in list_of_files_d){
file <- as.character(readtext(fileName)$text)
data <- rbind(data, c(file, 0))
}
}
colnames(data) <- c("text", "label")
return(data)
}
BiGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2,max=2))
convert_values <- function(x){
x <- ifelse(x>0, "yes", "no")
return(x)
}
# Load the train and test set
train.data <- loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 1, 4)
test.data <-loadData('./op_spam_v1.4/negative_polarity/truthful_from_Web', './op_spam_v1.4/negative_polarity/deceptive_from_MTurk', 5, 5)
#Turn data into a corpus
train_corpus <- VCorpus(VectorSource(train.data[,1]))
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_2/Assignment_2_code.R', echo=TRUE)
train_dtm_bi$dimnames$Terms
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, stopwords("english")) %>%
tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>%
tm_map(stripWhitespace)%>% tm_map(removeWords, c(stopwords("en"), "th", "us", "also"))
# For bigram
train_dtm_bi <- DocumentTermMatrix(train_corpus, control = list(tokenize = BiGramTokenizer))
train_dtm_bi$dimnames$Terms
dim(train_dtm_bi)
freq <- findFreqTerms(train_dtm_bi)
freq
freq <- findFreqTerms(train_dtm_bi, 15)
freq
freq <- findFreqTerms(train_dtm_bi, 5)
freq
train_dtm_bi <- DocumentTermMatrix(train_corpus, list(dictionary = fiveFreq))
fiveFreq <- findFreqTerms(train_dtm_bi, 5)
train_dtm_bi <- DocumentTermMatrix(train_corpus, list(dictionary = fiveFreq))
test_dtm_bi <- DocumentTermMatrix(test_corpus,control = list(tokenize = BiGramTokenizer, dictionary=dimnames(train_dtm_bi)[[2]]))
# Naive bayes classifier Bigram
NV_classifier_bi <- multinomial_naive_bayes(as.matrix(train_dtm_bi), as.factor(train.data[,2]))
NV_pred_bi <- predict(NV_classifier_bi, as.matrix(test_dtm_bi))
result_bi <- confusionMatrix(data = NV_pred_bi, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
result_bi$overall["Accuracy"]
result_bi$byClass[c("Precision", "Recall", "F1")]
train_dtm_bi <- DocumentTermMatrix(train_corpus, list(dictionary = fiveFreq))
train_dtm_bi$dimnames$Terms
test_dtm_bi <- DocumentTermMatrix(test_corpus,control = list(tokenize = BiGramTokenizer, dictionary=dimnames(train_dtm_bi)[[2]]))
test_dtm_bi$dimnames$Terms
# Naive bayes classifier Bigram
NV_classifier_bi <- multinomial_naive_bayes(as.matrix(train_dtm_bi), as.factor(train.data[,2]))
NV_pred_bi <- predict(NV_classifier_bi, as.matrix(test_dtm_bi))
result_bi <- confusionMatrix(data = NV_pred_bi, reference = as.factor(test.data[,2]), positive = "1",dnn = c("prediction", "actual"))
result_bi$overall["Accuracy"]
result_bi$byClass[c("Precision", "Recall", "F1")]
NV_pred_bi
NV_classifier_bi
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_2/Assignment_2_code.R', echo=TRUE)
train_corpus <- train_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, stopwords("english")) %>%
tm_map(lemmatize_words)%>%  tm_map(removePunctuation) %>%tm_map(removeNumbers) %>%
tm_map(stripWhitespace)%>% tm_map(removeWords, c(stopwords("en"), "th", "us"))
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_2/Assignment_2_code.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_2/Assignment_2_code_LR.R', echo=TRUE)
