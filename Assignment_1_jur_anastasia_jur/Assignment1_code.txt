library(data.tree)

# tree.grow
#   data: The dataset
#   y: The class labels of the dataset
#   nmin: The minimm length of the data in a node to not be a leaf node. 
#   minleaf: minleaf: The minimum number of datapoints that needs to be in a node to not be a leafnode
#   nfeat: The number of features that need to be sampled from the dataset
# Result: A data.tree object
# Description: Grows a tree datastructure
tree.grow <- function(data, y, nmin, minleaf, nfeat){
  
  # Set the rootnode with the full dataset row numbers and the all it's class labels
  root <- Node$new('start', dataX = data, dataY = y)
  
  # Convert the root into a list
  nodelist <- list(root)
  
  # While there are nodes in the nodes list expand the tree
  while(length(nodelist) > 0) {
    
    # Select the first node
    node <- nodelist[[1]]
    nodelist[[1]] <- NULL
    
    # Only consider a split if the node is not already pure of it the lenght of the dataset is shorter than nmin
    # Else the node is a leaf node
    if (impurity.gini(node$dataY) > 0 && nrow(node$dataX) > nmin) {
      
      # The best split point as given by best split
      split <- get.split(node$dataX, node$dataY, nfeat, minleaf)
      
      # # If there was no split found make the node a leaf node else create childeren based on the split
      if(is.null(split)) {
        isLeaf(node)
      } else {
        
        # Split the class labels into corresponding to the left and right data
        left_y <- node$dataY[node$dataX[,split$attribute] > split$splitpoint]
        right_y <- node$dataY[node$dataX[,split$attribute] <= split$splitpoint]

        # Set the split attribute and value for the current node and create the childeren
        node$Set(splitAttribute = split$attribute, splitValue = split$splitpoint)
        leftchild <- node$AddChild('leftChild', dataX = split$left_data, dataY = left_y)
        rightchild <- node$AddChild('rightChild', dataX = split$right_data, dataY = right_y)
        
        # Add the childeren to the leaf node
        nodelist <- append(nodelist, list(leftchild, rightchild))
      }
    } else {
      to.leaf(node)
    }
  }
  return(root)
}

# tree.classify
#   data: The dataset that needs to have it's class labels predicted
#   tr: The data.tree structure build by tree.grow
# Result: A vector of class labels
# Description: Predicts the class labels for a given dataset and a tree structure
tree.classify <- function(data, tr) {
  y <- c(1:100)
  for (i in 1:nrow(data)) {
    node <- tr
    # While the node is not a leaf node (isLeaf() is a function of data.tree package)
    while(!isLeaf(node)) {
      # If the value of the data is larger then the split value go to left child if smaller go to right child
      if (data[i, node$splitAttribute] > node$splitValue) {
        node <- node$leftChild
      } else {
        node <- node$rightChild
      }
    }
    # Retrieve the class label and store at the corresponding spot in the y vector
    y[i] <- node$label
  }
  return(y)
}

# tree.grow.bag
#   data: The dataset
#   y: The class labels of the dataset
#   nmin: The minimm length of the data in a node to not be a leaf node. 
#   minleaf: The minimum number of datapoints that needs to be in a node to not be a leafnode
#   nfeat: The number of features that need to be sampled from the dataset
#   m: The number of trees that need to be grown
# Result: A list of size m filled with data.tree objects
# Description: Multiple times grows a tree using tree.grow and stores them in a list
tree.grow.bag <- function(data, y, nmin, minleaf, nfeat, m) {
  trees <- c()
  for (i in 1:m) {
    
    # Sample  from the rows of the data to create a distinct datasets
    samples <- sample(nrow(data), nrow(data), TRUE)
    
    # Give the samples to tree.grow as the row numbers of the dataset
    trees <- append(trees, tree.grow(data[samples,], y[samples], nmin, minleaf, nfeat))
  }
  return(trees)
}

# tree.classify.bag
#   data: The dataset that needs to have it's class labels predicted
#   tr: A list of containing m data.tree structures created by tree.grow.bag
# Result: A vector of class labels
# Description: Classifies the data using a list of trees generated by tree.grow.bag
tree.classify.bag <- function(data, tr) {
  
  # Create a matrix for the predictions of each tree
  labels <- matrix(nrow=nrow(data), ncol=length(tr))
  predictions <- c()
  # Classify the dataset for each tree in tr and put the predictions in the right column of the labels matrix
  for (i in 1:length(tr)) {
    labels[,i] <- tree.classify(data, tr[[i]])
  }
  
  # Apply the mode function to each row to calculate the most predicted label from the m trees
  predictions <- apply(labels, 1, mode)
  return(predictions)
}

# impurity.gini
#   values: A binary vector
# Result: the impurity value, float
# Description: Calculate the impurity of a node using the Gini index
impurity.gini <- function(values) {
  len <- length(values)
  pos <- sum(values == 1)/len
  neg <- 1-pos
  return(neg*pos)
}

# mode
#   values: a vector of binary class values
# Result: the class label, boolean
# Description: Calculate the class label
mode <- function(values) return(sum(values)>(0.5*length(values)))

# best.split
#   data: The rows of the dataset that are in this node
#   y: The class labels for the data
#   minleaf: The minimum number of datapoints that needs to be in a node to not be a leafnode
# Result: the best splitpoint and corresponding impurity reduction, named list
# Description: Returns the best split for an attribute of the dataset
best.split <- function(data, y, minleaf) {
  
  # Find the possible split points for the data
  x_unique <- unique(data)
  x_sorted <- sort(x_unique)
  x_length <- length(x_sorted)
  
  # If there is only one split point a split is not possible, since the length of the data is too small
  if (length(x_sorted) < 2) {
    return(NULL)
  }
  x_splitpoints <- (x_sorted[1:(x_length-1)] + x_sorted[2:x_length])/2
  
  # Calculate impurity of the parent node
  parent_i <- impurity.gini(y)
  best_splitpoint <- -999
  best_reduction <- -999
  
  # Loop over the data to determine the best split point based on the highest impurity reduction
  for (val in x_splitpoints) {
    left_split <- y[data > val]
    right_split <- y[data <= val]
    
    # Only consider splits with that meet the minleaf constraint, others are not allowed
    if (length(right_split) >= minleaf && length(left_split) >= minleaf) {
      left_imp <- impurity.gini(left_split)
      right_imp <- impurity.gini(right_split)
      reduction_imp <- parent_i - ((right_imp * (length(right_split)/length(data)))+(left_imp*(length(left_split)/length(data))))
      if (reduction_imp > best_reduction) {
        best_splitpoint <- val
        best_reduction <- reduction_imp
      }
    }
  }
  # If there was no split found
  if (best_splitpoint == -999) {
    return(NULL)
  }
  return(list('best_reduction' = best_reduction, 'best_splitpoint'= best_splitpoint))
}

# get.split
#   data: the rows of the dataset corresponding to the node
#   y: a binary vector with the class labels corresponding to the data
#   nfeat: The number of features that need to be considerd by the sampeling for predictors
#   minleaf: The minimum number of datapoints that needs to be in a node to not be a leaf node
# Result: The best split, named list containing -> left_data, right_data, split_point and split attribute
# Description: Determine the best split over all of the attributes at a certain node
get.split <- function(data, y, nfeat, minleaf) {
  b_gini <- -999
  b_split <- NULL
  predictors <- sample(ncol(data), nfeat)
  colnames_data <- colnames(data)
  # Loop over the sampled predictors until the best split is found
  for (i in predictors){
    split <- best.split(data[,i], y, minleaf)
    
    # If there was no split found go to next predictor
    if (is.null(split)) next
    
    # The best split is the split with best reduction
    if (split$best_reduction > b_gini){
      b_gini <-  split$best_reduction
      b_attribute <- colnames_data[i]
      b_splitpoint <- split$best_splitpoint
    }
  }
  left <- data[data[,b_attribute] > b_splitpoint,]
  right <- data[data[,b_attribute] <= b_splitpoint,]
  b_split <- list('attribute'= b_attribute, 'splitpoint'= b_splitpoint, 'left_data' = left, 'right_data'= right)
  return(b_split)
}

# to.leaf
#   node: the node that needs to have it's class label set
# Result: setted class label for given node, boolean
# Description: Sets a class labels for a node if it becomes a leaf node
to.leaf <- function(node) node$Set(label = mode(node$dataY))
