test_classes <- test_data[,4] > 0
# Take relevant columns from data
train_data <- train_data[,c(3, 5:44)]
test_data <- test_data[,c(3, 5:44)]
# Convert the columns to numeric data to prevent errors
for (col in 1:ncol(train_data)){
train_data[,col] <- as.numeric(as.character(train_data[,col]))
test_data[,col] <- as.numeric(as.character(test_data[,col]))
}
get.split(train_data[1:10,], train_classes[1:10], 2, 1)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
print(x)
# Determine the best split over all of the attributes at a certain node
#   x: the rows of the dataset corresponding to the node
#   y: a binary vector with the class labels corresponding to x_row in the dataset
#   nfeat: The number of features that need to be considerd by the sampeling for predictors
#   minleaf: The minimum number of datapoints that needs to be in a node to not be a leaf node
get.split <- function(x, y, nfeat, minleaf) {
print(x)
b_gini <- -999
b_split <- NULL
predictors <- sample(ncol(x), nfeat)
# Loop over the sampled predictors until the best split is found
for (i in predictors){
split <- best.split(x[,i], y, minleaf)
# If there was no split found go to next predictor
if (is.null(split)) next
# Unlist the impurity value since we don't need it further in the program
gini <- unlist(split$best_reduction)
# The best split is the split with best reduction
if (gini > b_gini){
b_gini <- gini
b_split <- append(list('attribute'= i), split[2:4])
}
}
return(b_split)
}
get.split(train_data[1:10,], train_classes[1:10], 2, 1)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
print(x[1:nrow(x),b_attribute])
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
tree.grow(train_data[1:10,], train_classes[1:10], 2, 1, ncol(train_data))
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
# print(split$left_data)
# Set the split attribute and value for the current node and create the childeren
node$Set(splitAttribute = split$attribute, splitValue = split$splitpoint)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
# Split the class labels into corresponding to the left and right data
left_y <- y[node$dataX[,split$attribute] > split$splitpoint]
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
print(train_classes[1:10])
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
print(tree.grow(train_data[1:10,], train_classes[1:10], 2, 1, ncol(train_data)), 'splitAttribute', 'splitValue')
print(tree.grow(train_data, train_classes, 15, 5, ncol(train_data)), 'splitAttribute', 'splitValue')
print(tree.grow(train_data, train_classes, 15, 5, ncol(train_data)), 'splitAttribute', 'splitValue')
print(tree.grow(train_data[1:10,], train_classes[1:10], 1, 1, ncol(train_data)), 'splitAttribute', 'splitValue', 'dataX')
print(tree.grow(train_data[1:10,], train_classes[1:10], 2, 1, ncol(train_data)), 'splitAttribute', 'splitValue', 'dataX')
print(tree.grow(train_data[1:10,], train_classes[1:10], 2, 1, ncol(train_data)), 'splitAttribute', 'splitValue')
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
# print(tree.grow(train_data[1:10,], train_classes[1:10], 2, 1, ncol(train_data)), 'splitAttribute', 'splitValue')
# Grow a single tree, since the dataset is global only row numbers ar given to tree.grow
tree_1 <- tree.grow(train_data, train_classes, 15, 5, ncol(train_data))
# Prints the tree in the correct way
print(tree_1, 'splitValue', "splitAttribute")
predictions_1 <- tree.classify(test_data, tree_1)
# Predicts the class labels for a given dataset and a tree structure
#   data: The dataset that needs to have it's class labels predicted
#   tr: The data.tree structure build by tree.grow
tree.classify <- function(data, tr) {
y <- c(1:100)
for (i in 1:nrow(data)) {
node <- tr
# While the node is not a leaf node, (isLeaf() is a function of data.tree package)
while(!isLeaf(node)) {
# If the value of the data is larger then the split value go to left child if smaller go to right child
if (data[i, node$splitAttribute] > node$splitValue) {
node <- node$leftChild
} else {
node <- node$rightChild
}
}
# Retrieve the class label and store at the corresponding spot in the y vector
y[i] <- node$label
}
return(y)
}
predictions_1 <- tree.classify(test_data, tree_1)
# Prints the confusion matrix
print(table(predictions_1, test_classes))
# Gives the precision, recall and accuracy for a two binary vectors
#   y: the golden standard labels
#   predictions: the predictions made by one of the tree.classify functions
measures <- function(y, predictions) {
TP <- 0
FP <- 0
TN <- 0
FN <- 0
for (i in 1:length(predictions)){
if (predictions[i] == 1 && y[i] == 1) TP <- TP + 1
if (predictions[i] == 1 && y[i] == 0) FP <- FP + 1
if (predictions[i] == 0 && y[i] == 0) TN <- TN + 1
if (predictions[i] == 0 && y[i] == 1) FN <- FN + 1
}
precision <- TP/(TP+FP)
recall <- TP/(TP+FN)
accuracy <- (TP+TN)/length(predictions)
return(list('precision'= precision, 'recall'= recall, 'accuracy'= accuracy))
}
measures_1 <- measures(test_classes, predictions_1)
print(measures_1)
# print(tree.grow(train_data[1:10,], train_classes[1:10], 2, 1, ncol(train_data)), 'splitAttribute', 'splitValue')
# Grow a single tree, since the dataset is global only row numbers ar given to tree.grow
tree_1 <- tree.grow(train_data, train_classes, 15, 5, ncol(train_data))
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code.R', echo=TRUE)
# Grow a single tree, since the dataset is global only row numbers ar given to tree.grow
tree_1 <- tree.grow(c(1:nrow(train_data)), train_classes, 15, 5, 41)
# Prints the tree in the correct way
print(tree_1, 'splitValue', "splitAttribute")
# Prints the tree in the correct way
print(tree_1, 'splitValue', "splitAttribute" , 'label')
# Prints the tree in the correct way
print(tree_1, 'splitValue', "splitAttribute")
# Predicts the class labels for a given dataset and a tree structure
#   data: The dataset that needs to have it's class labels predicted
#   tr: The data.tree structure build by tree.grow
tree.classify <- function(data, tr) {
y <- c(1:100)
print(data)
for (i in 1:nrow(data)) {
node <- tr
# While the node is not a leaf node, (isLeaf() is a function of data.tree package)
while(!isLeaf(node)) {
# If the value of the data is larger then the split value go to left child if smaller go to right child
if (data[i, node$splitAttribute] > node$splitValue) {
node <- node$leftChild
} else {
node <- node$rightChild
}
}
# Retrieve the class label and store at the corresponding spot in the y vector
y[i] <- node$label
}
return(y)
}
predictions_1 <- tree.classify(test_data, tree_1)
# Predicts the class labels for a given dataset and a tree structure
#   data: The dataset that needs to have it's class labels predicted
#   tr: The data.tree structure build by tree.grow
tree.classify <- function(data, tr) {
y <- c(1:100)
for (i in 1:nrow(data)) {
node <- tr
print(node)
# While the node is not a leaf node, (isLeaf() is a function of data.tree package)
while(!isLeaf(node)) {
# If the value of the data is larger then the split value go to left child if smaller go to right child
if (data[i, node$splitAttribute] > node$splitValue) {
node <- node$leftChild
} else {
node <- node$rightChild
}
}
# Retrieve the class label and store at the corresponding spot in the y vector
y[i] <- node$label
}
return(y)
}
predictions_1 <- tree.classify(test_data, tree_1)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
# Predicts the class labels for a given dataset and a tree structure
#   data: The dataset that needs to have it's class labels predicted
#   tr: The data.tree structure build by tree.grow
tree.classify <- function(data, tr) {
y <- c(1:100)
for (i in 1:nrow(data)) {
node <- tr
# While the node is not a leaf node, (isLeaf() is a function of data.tree package)
while(!isLeaf(node)) {
print(data[i, node$splitAttribute] > node$splitValue)
# If the value of the data is larger then the split value go to left child if smaller go to right child
if (data[i, node$splitAttribute] > node$splitValue) {
node <- node$leftChild
} else {
node <- node$rightChild
}
}
# Retrieve the class label and store at the corresponding spot in the y vector
y[i] <- node$label
}
return(y)
}
predictions_1 <- tree.classify(test_data, tree_1)
measures_1 <- measures(test_classes, predictions_1)
print(measures_1)
# Prints the confusion matrix
print(table(predictions_1, test_classes))
# Predicts the class labels for a given dataset and a tree structure
#   data: The dataset that needs to have it's class labels predicted
#   tr: The data.tree structure build by tree.grow
tree.classify <- function(data, tr) {
y <- c(1:100)
for (i in 1:nrow(data)) {
node <- tr
# While the node is not a leaf node, (isLeaf() is a function of data.tree package)
while(!isLeaf(node)) {
print(data[i, node$splitAttribute] > node$splitValue)
# If the value of the data is larger then the split value go to left child if smaller go to right child
if (data[i, node$splitAttribute] > node$splitValue) {
print('left')
node <- node$leftChild
} else {
print('right')
node <- node$rightChild
}
}
# Retrieve the class label and store at the corresponding spot in the y vector
y[i] <- node$label
}
return(y)
}
predictions_1 <- tree.classify(test_data, tree_1)
# print(tree.grow(train_data[1:10,], train_classes[1:10], 2, 1, ncol(train_data)), 'splitAttribute', 'splitValue')
# Grow a single tree, since the dataset is global only row numbers ar given to tree.grow
tree_1 <- tree.grow(train_data, train_classes, 15, 5, ncol(train_data))
# Prints the tree in the correct way
print(tree_1, 'splitValue', "splitAttribute")
# Prints the tree in the correct way
print(tree_1, "splitAttribute", 'label')
# Grow a single tree, since the dataset is global only row numbers ar given to tree.grow
tree_1 <- tree.grow(c(1:nrow(train_data)), train_classes, 15, 5, 41)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code.R', echo=TRUE)
# Grow a single tree, since the dataset is global only row numbers ar given to tree.grow
tree_1 <- tree.grow(c(1:nrow(train_data)), train_classes, 15, 5, 41)
# Prints the tree in the correct way
print(tree_1, "splitAttribute", "label")
# print(tree.grow(train_data[1:10,], train_classes[1:10], 2, 1, ncol(train_data)), 'splitAttribute', 'splitValue')
# Grow a single tree, since the dataset is global only row numbers ar given to tree.grow
tree_1 <- tree.grow(train_data[1:50,], train_classes[1:50], 15, 5, ncol(train_data))
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
# Predicts the class labels for a given dataset and a tree structure
#   data: The dataset that needs to have it's class labels predicted
#   tr: The data.tree structure build by tree.grow
tree.classify <- function(data, tr) {
y <- c(1:100)
for (i in 1:nrow(data)) {
node <- tr
# While the node is not a leaf node, (isLeaf() is a function of data.tree package)
while(!isLeaf(node)) {
# If the value of the data is larger then the split value go to left child if smaller go to right child
if (data[i, node$splitAttribute] > node$splitValue) {
print('left')
node <- node$leftChild
} else {
print('right')
node <- node$rightChild
}
}
# Retrieve the class label and store at the corresponding spot in the y vector
y[i] <- node$label
}
return(y)
}
# print(tree.grow(train_data[1:10,], train_classes[1:10], 2, 1, ncol(train_data)), 'splitAttribute', 'splitValue')
# Grow a single tree, since the dataset is global only row numbers ar given to tree.grow
tree_1 <- tree.grow(train_data[1:50,], train_classes[1:50], 15, 5, ncol(train_data))
# Prints the tree in the correct way
print(tree_1, "splitAttribute", 'label')
# Grows a tree datastructure
#   x: The dataset
#   y: The class labels of the dataset
#   nmin: The minimm length of the data in a node to not be a leaf node.
#   minleaf: minleaf: The minimum number of datapoints that needs to be in a node to not be a leafnode
#   nfeat: The number of features that need to be sampled from the dataset
tree.grow <- function(x, y, nmin, minleaf, nfeat){
# Set the rootnode with the full dataset row numbers and the all it's class labels
root <- Node$new('start', dataX = x, dataY = y)
# Convert the root into a list
nodelist <- list(root)
# While there are nodes in the nodes list expand the tree
while(length(nodelist) > 0) {
# Select the first node
node <- nodelist[[1]]
nodelist[[1]] <- NULL
# Only consider a split if the node is not already pure of it the lenght of the dataset is shorter than nmin
# Else the node is a leaf node
if (impurity.gini(node$dataY) > 0 && nrow(node$dataX) > nmin) {
# The best split point as given by best split
split <- get.split(node$dataX, node$dataY, nfeat, minleaf)
# # If there was no split found make the node a leaf node else create childeren based on the split
if(is.null(split)) {
isLeaf(node)
} else {
# Split the class labels into corresponding to the left and right data
left_y <- y[node$dataX[,split$attribute] > split$splitpoint]
right_y <- y[node$dataX[,split$attribute] <= split$splitpoint]
print(left_y)
print(node$dataX[,split$attribute] > split$splitpoint)
# Set the split attribute and value for the current node and create the childeren
node$Set(splitAttribute = split$attribute, splitValue = split$splitpoint)
leftchild <- node$AddChild('leftChild', dataX = split$left_data, dataY = left_y)
rightchild <- node$AddChild('rightChild', dataX = split$right_data, dataY = right_y)
# Add the childeren to the leaf node
nodelist <- append(nodelist, list(leftchild, rightchild))
}
} else {
to.leaf(node)
}
}
return(root)
}
# print(tree.grow(train_data[1:10,], train_classes[1:10], 2, 1, ncol(train_data)), 'splitAttribute', 'splitValue')
# Grow a single tree, since the dataset is global only row numbers ar given to tree.grow
tree_1 <- tree.grow(train_data[1:50,], train_classes[1:50], 15, 5, ncol(train_data))
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
# Set the split attribute and value for the current node and create the childeren
node$Set(splitAttribute = split$attribute, splitValue = split$splitpoint)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
# print(tree.grow(train_data[1:10,], train_classes[1:10], 2, 1, ncol(train_data)), 'splitAttribute', 'splitValue')
# Grow a single tree, since the dataset is global only row numbers ar given to tree.grow
tree_1 <- tree.grow(train_data, train_classes, 15, 5, ncol(train_data))
# Prints the tree in the correct way
print(tree_1, "splitAttribute", 'label')
predictions_1 <- tree.classify(test_data, tree_1)
measures_1 <- measures(test_classes, predictions_1)
print(measures_1)
# Prints the confusion matrix
print(table(predictions_1, test_classes))
# Multiple times grows a tree using tree.grow and stores them in a list
#   x: The dataset
#   y: The class labels of the dataset
#   nmin: The minimm length of the data in a node to not be a leaf node.
#   minleaf: The minimum number of datapoints that needs to be in a node to not be a leafnode
#   nfeat: The number of features that need to be sampled from the dataset
#   m: The number of trees that need to be grown
tree.grow.bag <- function(x, y, nmin, minleaf, nfeat, m) {
trees <- c()
for (i in 1:m) {
# Sample  from the rows of x to create a distinct datasets
samples <- sample(nrow(x), nrow(x), TRUE)
# Give the samples to tree.grow as the row numbers of the dataset
trees <- append(trees, tree.grow(x[samples,], y[samples], nmin, minleaf, nfeat))
}
return(trees)
}
# Classifies the data using a list of trees generated by tree.grow.bag
#   data: The dataset that needs to have it's class labels predicted
#   tr: A list of containing m data.tree structures created by tree.grow.bag
tree.classify.bag <- function(data, tr) {
# Create a matrix for the predictions of each tree
labels <- matrix(nrow=nrow(data), ncol=length(tr))
predictions <- c()
# Classify the dataset for each tree in tr and put the predictions in the right column of the labels matrix
for (i in 1:length(tr)) {
labels[,i] <- tree.classify(data, tr[[i]])
}
# Calculate the final class label for each row based on what each tree predicted
for(i in 1:nrow(labels)) {
predictions[i] <- sum(labels[i,])>(0.5*length(labels[i,]))
}
return(predictions)
}
# Grow trees using random forest and classify
trees_1 <- tree.grow.bag(c(1:nrow(train_data)), train_classes, 15, 5, 41, 100)
# Grow trees using random forest and classify
trees_1 <- tree.grow.bag(train_data, train_classes, 15, 5, 41, 100)
# print(trees_1[[1]], "splitValue", 'splitAttribute')
predictions_2 <- tree.classify.bag(test_data, trees_1)
measures_2 <- measures(test_classes, predictions_2)
print(measures_2)
print(table(predictions_2, test_classes))
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
# Determine the best split over all of the attributes at a certain node
#   data: the rows of the dataset corresponding to the node
#   y: a binary vector with the class labels corresponding to the data
#   nfeat: The number of features that need to be considerd by the sampeling for predictors
#   minleaf: The minimum number of datapoints that needs to be in a node to not be a leaf node
get.split <- function(data, y, nfeat, minleaf) {
b_gini <- -999
b_split <- NULL
predictors <- sample(ncol(data), nfeat)
print(predictors)
colnames_data <- colnames(data)
# Loop over the sampled predictors until the best split is found
for (i in predictors){
split <- best.split(data[,i], y, minleaf)
# If there was no split found go to next predictor
if (is.null(split)) next
# Unlist the impurity value since we don't need it further in the program
gini <- unlist(split$best_reduction)
# The best split is the split with best reduction
if (gini > b_gini){
b_gini <- gini
b_attribute <- colnames_data[i]
b_splitpoint <- split$best_splitpoint
}
}
left <- data[data[,b_attribute] > b_splitpoint,]
right <- data[data[,b_attribute] <= b_splitpoint,]
b_split <- list('attribute'= b_attribute, 'splitpoint'= b_splitpoint, 'left_data' = left, 'right_data'= right)
return(b_split)
}
# Grow trees using bagging and classify
trees_2 <- tree.grow.bag(train_data, train_classes, 15, 5, 6, 100)
# Determine the best split over all of the attributes at a certain node
#   data: the rows of the dataset corresponding to the node
#   y: a binary vector with the class labels corresponding to the data
#   nfeat: The number of features that need to be considerd by the sampeling for predictors
#   minleaf: The minimum number of datapoints that needs to be in a node to not be a leaf node
get.split <- function(data, y, nfeat, minleaf) {
b_gini <- -999
b_split <- NULL
predictors <- sample(ncol(data), nfeat)
colnames_data <- colnames(data)
# Loop over the sampled predictors until the best split is found
for (i in predictors){
split <- best.split(data[,i], y, minleaf)
# If there was no split found go to next predictor
if (is.null(split)) next
# Unlist the impurity value since we don't need it further in the program
gini <- unlist(split$best_reduction)
# The best split is the split with best reduction
if (gini > b_gini){
b_gini <- gini
b_attribute <- colnames_data[i]
b_splitpoint <- split$best_splitpoint
}
}
left <- data[data[,b_attribute] > b_splitpoint,]
right <- data[data[,b_attribute] <= b_splitpoint,]
b_split <- list('attribute'= b_attribute, 'splitpoint'= b_splitpoint, 'left_data' = left, 'right_data'= right)
return(b_split)
}
# Grow trees using bagging and classify
trees_2 <- tree.grow.bag(train_data, train_classes, 15, 5, 6, 100)
print(trees_2[[1]], "splitValue", 'splitAttribute', 'label')
predictions_3 <- tree.classify.bag(test_data, trees_2)
# print(trees_1[[1]], "splitValue", 'splitAttribute', 'labels)
predictions_2 <- tree.classify.bag(test_data, trees_1)
print(predictions_2)
# Classifies the data using a list of trees generated by tree.grow.bag
#   data: The dataset that needs to have it's class labels predicted
#   tr: A list of containing m data.tree structures created by tree.grow.bag
tree.classify.bag <- function(data, tr) {
# Create a matrix for the predictions of each tree
labels <- matrix(nrow=nrow(data), ncol=length(tr))
predictions <- c()
# Classify the dataset for each tree in tr and put the predictions in the right column of the labels matrix
for (i in 1:length(tr)) {
labels[,i] <- tree.classify(data, tr[[i]])
}
# # Calculate the final class label for each row based on what each tree predicted
# for(i in 1:nrow(labels)) {
#   predictions[i] <- sum(labels[i,])>(0.5*length(labels[i,]))
# }
predictions <- apply(labels, 2, mode)
return(predictions)
}
# print(trees_1[[1]], "splitValue", 'splitAttribute', 'labels)
predictions_2 <- tree.classify.bag(test_data, trees_1)
print(predictions_2)
# Classifies the data using a list of trees generated by tree.grow.bag
#   data: The dataset that needs to have it's class labels predicted
#   tr: A list of containing m data.tree structures created by tree.grow.bag
tree.classify.bag <- function(data, tr) {
# Create a matrix for the predictions of each tree
labels <- matrix(nrow=nrow(data), ncol=length(tr))
predictions <- c()
# Classify the dataset for each tree in tr and put the predictions in the right column of the labels matrix
for (i in 1:length(tr)) {
labels[,i] <- tree.classify(data, tr[[i]])
}
# # Calculate the final class label for each row based on what each tree predicted
# for(i in 1:nrow(labels)) {
#   predictions[i] <- sum(labels[i,])>(0.5*length(labels[i,]))
# }
predictions <- apply(labels, 1, mode)
return(predictions)
}
# print(trees_1[[1]], "splitValue", 'splitAttribute', 'labels)
predictions_2 <- tree.classify.bag(test_data, trees_1)
print(predictions_2)
measures_2 <- measures(test_classes, predictions_2)
print(measures_2)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
source('C:/Users/Lisa/Desktop/UU/Data_Mining_2019/Assignment_1_code_v2.R', echo=TRUE)
